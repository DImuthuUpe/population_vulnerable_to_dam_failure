{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd0a17b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jparkgeo/opt/anaconda3/envs/agingdam/lib/python3.9/site-packages/qinfer/parallel.py:51: UserWarning: Could not import IPython parallel. Parallelization support will be disabled.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import requests\n",
    "import esda\n",
    "import libpysal\n",
    "import multiprocessing as mp\n",
    "import itertools\n",
    "import qinfer\n",
    "import shapely\n",
    "import numpy as np\n",
    "import pygeos\n",
    "import numpy.linalg as la\n",
    "import shapely.affinity\n",
    "import subprocess\n",
    "from osgeo import gdal\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c55a5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_raster(rasterfile_path, filename, target_path, rescale_factor):\n",
    "    # first determine pixel size for resampling\n",
    "    xres = 0\n",
    "    yres = 0\n",
    "    \n",
    "    out = subprocess.run([\"gdalinfo\",\"-json\",rasterfile_path],stdout=subprocess.PIPE)\n",
    "    raster_meta = json.loads(out.stdout.decode('utf-8'))\n",
    "    if 'geoTransform' in raster_meta:\n",
    "        xres = raster_meta['geoTransform'][1]\n",
    "        yres = raster_meta['geoTransform'][5]\n",
    "        xres = xres * rescale_factor\n",
    "        yres = yres * rescale_factor\n",
    "\n",
    "    if (xres != 0) and (yres != 0):\n",
    "        # resample raster\n",
    "        save_path = target_path +\"/\"+ filename + f\"_resample_{rescale_factor}.tiff\"\n",
    "        subprocess.run([\"gdalwarp\",\"-r\",\"bilinear\",\"-of\",\"GTiff\",\"-tr\",str(xres),str(yres),rasterfile_path,save_path])\n",
    "\n",
    "        return save_path\n",
    "    \n",
    "def polygonize_fim(rasterfile_path):\n",
    "\n",
    "    # Extract target path and filename from the given raster file path\n",
    "    target_path = '/'.join(rasterfile_path.split('/')[:-1])\n",
    "    filename = rasterfile_path.split(\"/\")[-1].split(\".\")[-2]\n",
    "\n",
    "    # Resample raster file to 10-times smaller\n",
    "    resample_10_path = resample_raster(rasterfile_path, filename, target_path, rescale_factor=10)\n",
    "\n",
    "    # Reclassify raster\n",
    "    '''\n",
    "    water_lvl = [0, 2, 6, 15, np.inf]  # Original inundation map value (underwater in feet)\n",
    "    water_lvl_recls = [-9999, 1, 2, 3, 4]\n",
    "    '''\n",
    "    reclass_file = target_path + \"/\" + filename + \"_reclass.tiff\"\n",
    "    outfile = \"--outfile=\"+reclass_file\n",
    "    subprocess.run([\"gdal_calc.py\",\"-A\",resample_10_path,outfile,\"--calc=-9999*(A<=0)+1*((A>0)*(A<=2))+2*((A>2)*(A<=6))+3*((A>6)*(A<=15))+4*(A>15)\",\"--NoDataValue=-9999\"],stdout=subprocess.PIPE)\n",
    "\n",
    "    # Polygonize the reclassified raster\n",
    "    geojson_out = \"%s/%s.json\" % (target_path, filename)\n",
    "    subprocess.run([\"gdal_polygonize.py\", reclass_file, \"-b\", \"1\", geojson_out, filename, \"value\"])\n",
    "\n",
    "    inund_polygons = gpd.read_file(geojson_out)\n",
    "    inund_polygons = inund_polygons.loc[(inund_polygons['value'] != -9999) & (inund_polygons['value'] != 0)]  # Remove pixels of null value\n",
    "\n",
    "    # drop invalid geometries\n",
    "    inund_polygons = inund_polygons.loc[inund_polygons['geometry'].is_valid, :]\n",
    "\n",
    "    # Coverage for each class of inundation map\n",
    "    inund_per_cls = inund_polygons.dissolve(by='value')\n",
    "    inund_per_cls.reset_index(inplace=True)\n",
    "\n",
    "    # remove all temp files\n",
    "    os.remove(resample_10_path)\n",
    "    os.remove(reclass_file)\n",
    "    os.remove(geojson_out)\n",
    "\n",
    "    # inundation_per_cls: GeoDataFrame \n",
    "    return inund_per_cls\n",
    "\n",
    "def fim_and_ellipse(dam_id, scene, input_dir):\n",
    "        \n",
    "    fim_path = f\"./{input_dir}/{scenarios['loadCondition']}_{scenarios['breachCondition']}_{dam_id}.tiff\"\n",
    "    \n",
    "    fim_gdf = polygonize_fim(fim_path)\n",
    "    fim_gdf['Dam_ID'] = dam_id\n",
    "    fim_gdf['Scenario'] = f\"{scene['loadCondition']}_{scene['breachCondition']}\"\n",
    "        \n",
    "    return fim_gdf\n",
    "\n",
    "def state_num_related_to_fim(fim_gdf, tract_gdf):\n",
    "    \n",
    "    tract_geoms = pygeos.from_shapely(tract_gdf['geometry'].values)\n",
    "    tract_geoms_tree = pygeos.STRtree(tract_geoms, leafsize=50)\n",
    "\n",
    "    fim_geom_union = pygeos.from_shapely(fim_gdf['geometry'].unary_union)    \n",
    "    query_intersect = tract_geoms_tree.query(fim_geom_union, predicate='intersects')\n",
    "    tract_gdf = tract_gdf.loc[query_intersect]\n",
    "\n",
    "    tract_gdf['STATE'] = tract_gdf.apply(lambda x:x['GEOID'][0:2], axis=1)\n",
    "    unique_state = tract_gdf['STATE'].unique()\n",
    "    \n",
    "    # return type: list\n",
    "    return unique_state\n",
    "    \n",
    "def extract_fim_geoid(dam_id, scene, input_dir, tract_gdf):\n",
    "    print(f'{dam_id}: Step 1, 1/4, Identifying associated regions (Ellipse)')\n",
    "    fim_gdf = fim_and_ellipse(dam_id, scene, input_dir)\n",
    "\n",
    "    print(f'{dam_id}: Step 1, 2/4, Search states associated')\n",
    "    fim_state = state_num_related_to_fim(fim_gdf, tract_gdf)\n",
    "    print(f'-- {dam_id} impacts {len(fim_state)} States, {fim_state}')\n",
    "\n",
    "    if len(fim_state) == 1: # If only one state is associated with the inundation mapping\n",
    "        census_gdf = gpd.read_file(f'./census_geometry/tl_2020_{fim_state[0]}_tabblock20.geojson')\n",
    "    elif len(fim_state) >= 2: # If multiple states are associated with the inundation mapping\n",
    "        census_gdf = pd.DataFrame()\n",
    "        for state_num in fim_state:\n",
    "            temp_gdf = gpd.read_file(f'./census_geometry/tl_2020_{state_num}_tabblock20.geojson')\n",
    "            census_gdf = pd.concat([temp_gdf, census_gdf]).reset_index(drop=True)\n",
    "            census_gdf = gpd.GeoDataFrame(census_gdf, geometry=census_gdf['geometry'], crs=\"EPSG:4326\")\n",
    "    else:\n",
    "        raise AttributeError('NO STATE is related to Inundation Mapping')\n",
    "\n",
    "    # Destination dataframe to save the results\n",
    "    print(f\"{dam_id}: Step 1, 3/4, Extracting GEOID of census blocks\")\n",
    "    fim_geoid_df = pd.DataFrame({'Dam_ID': pd.Series(dtype='str'),\n",
    "                                'Scenario': pd.Series(dtype='str'),\n",
    "                                'GEOID': pd.Series(dtype='str'),\n",
    "                                'Class': pd.Series(dtype='str')}\n",
    "                                )    \n",
    "\n",
    "    # Create STRtree for census_gdf\n",
    "    census_geoms = pygeos.from_shapely(census_gdf['geometry'].values)\n",
    "    census_geoms_tree = pygeos.STRtree(census_geoms, leafsize=50)\n",
    "\n",
    "    # Extract census tract intersecting with each class of inundation map\n",
    "    for water_cls in fim_gdf['value'].unique():\n",
    "        fim_geom_ = pygeos.from_shapely(fim_gdf.loc[fim_gdf['value'] == water_cls, 'geometry'].values[0])\n",
    "        query_fim_geom_ = census_geoms_tree.query(fim_geom_, predicate='intersects')\n",
    "        fim_geoid_ = census_gdf.loc[query_fim_geom_]\n",
    "\n",
    "        for geoid_ in fim_geoid_['GEOID'].to_list():\n",
    "            new_row = pd.DataFrame({'Dam_ID': dam_id, \n",
    "                                    'Scenario': f\"{scene['loadCondition']}_{scene['breachCondition']}\", \n",
    "                                    'GEOID': geoid_, \n",
    "                                    'Class': water_cls}, \n",
    "                                    index=[0]\n",
    "                                    )\n",
    "            fim_geoid_df = pd.concat([new_row, fim_geoid_df]).reset_index(drop=True)\n",
    "\n",
    "    print(f\"{dam_id}: Step 1, 4/4, Assigning geometry to census blocks\")\n",
    "    fim_geoid_gdf = fim_geoid_df.merge(census_gdf, on='GEOID')\n",
    "    fim_geoid_gdf = gpd.GeoDataFrame(fim_geoid_gdf, geometry=fim_geoid_gdf['geometry'], crs='EPSG:4326')\n",
    "    fim_geoid_gdf['Class'] = fim_geoid_gdf['Class'].astype(int)\n",
    "    fim_geoid_gdf = fim_geoid_gdf.groupby(['Dam_ID', 'Scenario', 'GEOID'], \n",
    "                                    group_keys=False).apply(lambda x:x.loc[x['Class'].idxmax()]\n",
    "                                                            ).reset_index(drop=True)\n",
    "    fim_geoid_gdf = fim_geoid_gdf.set_crs(epsg=4326)\n",
    "\n",
    "    return fim_geoid_gdf, fim_gdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dc9f28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "\n",
    "scenarios = {'loadCondition': 'MH', 'breachCondition': 'F'}\n",
    "input_dir = f'NID_FIM_{scenarios[\"loadCondition\"]}_{scenarios[\"breachCondition\"]}'\n",
    "output_dir = f'{scenarios[\"loadCondition\"]}_{scenarios[\"breachCondition\"]}_Results'\n",
    "API_Key = 'fbcac1c2cc26d853b42c4674adf905e742d1cb2b' # Census api key\n",
    "tract_gdf = gpd.read_file(os.path.join(cwd, 'census_geometry', 'census_tract_from_api.geojson'))\n",
    "\n",
    "# Find the list of dams in the input folder\n",
    "fed_dams = requests.get('https://fim.sec.usace.army.mil/ci/fim/getAllEAPStructure').json()\n",
    "fed_dams = pd.DataFrame(fed_dams)\n",
    "fed_dams = gpd.GeoDataFrame(fed_dams, geometry=gpd.points_from_xy(fed_dams['LON'], fed_dams['LAT'], crs=\"EPSG:4326\"))\n",
    "# dois = fed_dams['ID'].to_list()\n",
    "# dois = ['TX00009', 'TX00010', 'TX00011', 'TX00012'] \n",
    "dam_id = 'TX00019'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0aad087c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TX00019: Step 1, 1/4, Identifying associated regions (Ellipse)\n",
      "Creating output file that is 4033P x 3915L.\n",
      "Processing ./NID_FIM_MH_F/MH_F_TX00019.tiff [1/1] : 0Using internal nodata values (e.g. -9999) for image ./NID_FIM_MH_F/MH_F_TX00019.tiff.\n",
      "Copying nodata values from source ./NID_FIM_MH_F/MH_F_TX00019.tiff to destination ./NID_FIM_MH_F/MH_F_TX00019_resample_10.tiff.\n",
      "...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "0...10...20...30...40...50...60...70...80...90...Several drivers matching json extension. Using GeoJSON\n",
      "Creating output ./NID_FIM_MH_F/MH_F_TX00019.json of format GeoJSON.\n",
      "100 - done.\n",
      "TX00019: Step 1, 2/4, Search states associated\n",
      "-- TX00019 impacts 1 States, ['48']\n",
      "TX00019: Step 1, 3/4, Extracting GEOID of census blocks\n",
      "TX00019: Step 1, 4/4, Assigning geometry to census blocks\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dam_ID</th>\n",
       "      <th>Scenario</th>\n",
       "      <th>GEOID</th>\n",
       "      <th>Class</th>\n",
       "      <th>HOUSING20</th>\n",
       "      <th>POP20</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TX00019</td>\n",
       "      <td>MH_F</td>\n",
       "      <td>480396619021162</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>POLYGON ((-95.54983 29.43844, -95.54754 29.438...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TX00019</td>\n",
       "      <td>MH_F</td>\n",
       "      <td>480396619021165</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>POLYGON ((-95.55541 29.42554, -95.55237 29.427...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TX00019</td>\n",
       "      <td>MH_F</td>\n",
       "      <td>480396619021167</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>POLYGON ((-95.55123 29.43129, -95.55105 29.432...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TX00019</td>\n",
       "      <td>MH_F</td>\n",
       "      <td>480396619021168</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>POLYGON ((-95.55641 29.42461, -95.55594 29.425...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TX00019</td>\n",
       "      <td>MH_F</td>\n",
       "      <td>481576709042000</td>\n",
       "      <td>2</td>\n",
       "      <td>124</td>\n",
       "      <td>325</td>\n",
       "      <td>POLYGON ((-95.55332 29.55620, -95.55327 29.556...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21754</th>\n",
       "      <td>TX00019</td>\n",
       "      <td>MH_F</td>\n",
       "      <td>482019807001099</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>POLYGON ((-95.36492 29.75696, -95.36406 29.756...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21755</th>\n",
       "      <td>TX00019</td>\n",
       "      <td>MH_F</td>\n",
       "      <td>482019807001100</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>POLYGON ((-95.36551 29.75621, -95.36464 29.755...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21756</th>\n",
       "      <td>TX00019</td>\n",
       "      <td>MH_F</td>\n",
       "      <td>482019807001101</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>POLYGON ((-95.36640 29.75673, -95.36551 29.756...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21757</th>\n",
       "      <td>TX00019</td>\n",
       "      <td>MH_F</td>\n",
       "      <td>482019807001102</td>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>POLYGON ((-95.36698 29.75599, -95.36610 29.755...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21758</th>\n",
       "      <td>TX00019</td>\n",
       "      <td>MH_F</td>\n",
       "      <td>482019807001103</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>POLYGON ((-95.36610 29.75545, -95.36524 29.754...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21759 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Dam_ID Scenario            GEOID  Class  HOUSING20  POP20  \\\n",
       "0      TX00019     MH_F  480396619021162      2          0      0   \n",
       "1      TX00019     MH_F  480396619021165      2          0      0   \n",
       "2      TX00019     MH_F  480396619021167      2          0      0   \n",
       "3      TX00019     MH_F  480396619021168      2          0      0   \n",
       "4      TX00019     MH_F  481576709042000      2        124    325   \n",
       "...        ...      ...              ...    ...        ...    ...   \n",
       "21754  TX00019     MH_F  482019807001099      2          2      2   \n",
       "21755  TX00019     MH_F  482019807001100      2          0     19   \n",
       "21756  TX00019     MH_F  482019807001101      2          0      0   \n",
       "21757  TX00019     MH_F  482019807001102      2         52      0   \n",
       "21758  TX00019     MH_F  482019807001103      2          0      0   \n",
       "\n",
       "                                                geometry  \n",
       "0      POLYGON ((-95.54983 29.43844, -95.54754 29.438...  \n",
       "1      POLYGON ((-95.55541 29.42554, -95.55237 29.427...  \n",
       "2      POLYGON ((-95.55123 29.43129, -95.55105 29.432...  \n",
       "3      POLYGON ((-95.55641 29.42461, -95.55594 29.425...  \n",
       "4      POLYGON ((-95.55332 29.55620, -95.55327 29.556...  \n",
       "...                                                  ...  \n",
       "21754  POLYGON ((-95.36492 29.75696, -95.36406 29.756...  \n",
       "21755  POLYGON ((-95.36551 29.75621, -95.36464 29.755...  \n",
       "21756  POLYGON ((-95.36640 29.75673, -95.36551 29.756...  \n",
       "21757  POLYGON ((-95.36698 29.75599, -95.36610 29.755...  \n",
       "21758  POLYGON ((-95.36610 29.75545, -95.36524 29.754...  \n",
       "\n",
       "[21759 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "census_dic = {\n",
    "                \"EP_POV150\" : [['S1701_C01_040E'], 'S1701_C01_001E'],\n",
    "                \"EP_UNEMP\"  : 'DP03_0009PE',\n",
    "                \"EP_HBURD\"  : [['S2503_C01_028E', 'S2503_C01_032E', 'S2503_C01_036E', 'S2503_C01_040E'], \n",
    "                            'S2503_C01_001E'],\n",
    "                \"EP_NOHSDP\" : 'S0601_C01_033E',\n",
    "                \"EP_UNINSUR\" : 'S2701_C05_001E',\n",
    "                \"EP_AGE65\" : 'S0101_C02_030E',\n",
    "                \"EP_AGE17\" : [['B09001_001E'], \n",
    "                            'S0601_C01_001E'],\n",
    "                \"EP_DISABL\" : 'DP02_0072PE',\n",
    "                \"EP_SNGPNT\" : [['B11012_010E', 'B11012_015E'], 'DP02_0001E'],\n",
    "                \"EP_LIMENG\" : [['B16005_007E', 'B16005_008E', 'B16005_012E', 'B16005_013E', 'B16005_017E', 'B16005_018E', \n",
    "                                'B16005_022E', 'B16005_023E', 'B16005_029E', 'B16005_030E', 'B16005_034E', 'B16005_035E',\n",
    "                                'B16005_039E', 'B16005_040E', 'B16005_044E', 'B16005_045E'], \n",
    "                            'B16005_001E'],\n",
    "                \"EP_MINRTY\" : [['DP05_0071E', 'DP05_0078E', 'DP05_0079E', 'DP05_0080E', \n",
    "                                'DP05_0081E', 'DP05_0082E', 'DP05_0083E'],\n",
    "                            'S0601_C01_001E'],\n",
    "                \"EP_MUNIT\" : [['DP04_0012E', 'DP04_0013E'], \n",
    "                            'DP04_0001E'],\n",
    "                \"EP_MOBILE\" : 'DP04_0014PE',\n",
    "                \"EP_CROWD\" : [['DP04_0078E', 'DP04_0079E'], \n",
    "                            'DP04_0002E'],\n",
    "                \"EP_NOVEH\" : 'DP04_0058PE',\n",
    "                \"EP_GROUPQ\": [['B26001_001E'], \n",
    "                            'S0601_C01_001E'],\n",
    "}\n",
    "\n",
    "\n",
    "fim_geoid_gdf, fim_gdf = extract_fim_geoid(dam_id, scenarios, input_dir, tract_gdf)\n",
    "fim_geoid_gdf\n",
    "# mi_gdf, lm_gdf = spatial_correlation(dam_id, fed_dams, fim_geoid_gdf, census_dic, API_Key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa2f4389",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gaussian(dij, d0):  # Gaussian probability distribution\n",
    "    if d0 >= dij:\n",
    "        val = (math.exp(-1 / 2 * ((dij / d0) ** 2)) - math.exp(-1 / 2)) / (1 - math.exp(-1 / 2))\n",
    "        return val\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def gaussian_weights(gdf, d0):\n",
    "    d_neighbors = {}\n",
    "    d_weights = {}\n",
    "    \n",
    "    for i in range(gdf.shape[0]):\n",
    "        l_neighbors = []\n",
    "        l_weights = []\n",
    "        \n",
    "        for j in range(gdf.shape[0]):\n",
    "            if i != j:\n",
    "                temp_dist = gdf.at[i, 'geometry'].centroid.distance(gdf.at[j, 'geometry'].centroid)\n",
    "                \n",
    "                if temp_dist <= d0:\n",
    "            \n",
    "                    l_neighbors.append(j)\n",
    "                    l_weights.append(gaussian(temp_dist, d0))\n",
    "                    \n",
    "        d_neighbors[i] = l_neighbors\n",
    "        d_weights[i] = l_weights\n",
    "            \n",
    "    return d_neighbors, d_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0578bcbe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TX00019: Step 2, 1/2, Retrieving census data\n",
      "TX00019: Step 2, 2/2, Calculating Moran's I and LISA\n"
     ]
    }
   ],
   "source": [
    "def call_census_table(state_list, table_name, key):\n",
    "    \n",
    "    result_df = pd.DataFrame()\n",
    "    \n",
    "    # querying at census tract level\n",
    "    for state in state_list:\n",
    "        if table_name.startswith('DP'):\n",
    "            address = f'https://api.census.gov/data/2020/acs/acs5/profile?get=NAME,{table_name}&for=tract:*&in=state:{state}&in=county:*'\n",
    "        elif table_name.startswith('S'):\n",
    "            address = f'https://api.census.gov/data/2020/acs/acs5/subject?get=NAME,{table_name}&for=tract:*&in=state:{state}&in=county:*'\n",
    "        elif table_name.startswith('B'):\n",
    "            address = f'https://api.census.gov/data/2020/acs/acs5?get=NAME,{table_name}&for=tract:*&in=state:{state}&in=county:*'\n",
    "        else:\n",
    "            raise AttributeError('Proper Table Name Is Needed.')\n",
    "            \n",
    "        response = requests.get(f'{address}&key={key}').json()\n",
    "        result_ = pd.DataFrame(response)\n",
    "        \n",
    "        result_.columns = response[0]\n",
    "        result_.drop(0, axis=0, inplace=True)\n",
    "        \n",
    "        result_df = pd.concat([result_, result_df]).reset_index(drop=True)\n",
    "        \n",
    "    # result_df = result_df.rename(columns={'GEO_ID':'GEOID_T'})\n",
    "    result_df['GEOID_T'] = result_df.apply(lambda x: x['state'] + x['county'] + x['tract'], axis=1)\n",
    "    result_df[table_name] = result_df[table_name].astype(float)\n",
    "        \n",
    "    return result_df[['GEOID_T', table_name]]\n",
    "\n",
    "\n",
    "def calculate_bivariate_Moran_I_and_LISA(dam_id, census_dic, fim_geoid_gdf, dams_gdf):\n",
    "\n",
    "    input_cols = list(census_dic.keys())\n",
    "    input_cols.extend(['Dam_ID', 'GEOID', 'Class', 'geometry'])\n",
    "    fim_geoid_local = fim_geoid_gdf.loc[fim_geoid_gdf['Dam_ID'] == dam_id, input_cols].reset_index(drop=True)\n",
    "    dam_local = dams_gdf.loc[dams_gdf['ID'] == dam_id].reset_index(drop=True)\n",
    "\n",
    "    # Iterate through all census variables\n",
    "    for census_name in census_dic.keys():\n",
    "        new_col_name = census_name.split(\"_\")[1]\n",
    "        \n",
    "        # Local fim_geoid\n",
    "        fim_geoid_local_var = fim_geoid_local.loc[~fim_geoid_local[census_name].isna(), ['Dam_ID', 'GEOID', 'Class', census_name, 'geometry']].reset_index(drop=True)\n",
    "\n",
    "        # TODO: investigate proportional bandwidth or Kenel window for distance decay\n",
    "        # Calculate Bivaraite Moran's I & Local Moran's I\n",
    "#         w = libpysal.weights.Queen.from_dataframe(fim_geoid_local_var)  # Adjacency matrix (Queen case)\n",
    "\n",
    "        max_dist = int(fim_geoid_local_var.geometry.unary_union.convex_hull.length / (2 * 3.14))\n",
    "        dist_digit = len(str(max_dist))\n",
    "        # start_dist = int('1'.ljust(dist_digit-1, '0'))\n",
    "        start_dist = int(str(max_dist)[0].ljust(dist_digit-1, '0'))\n",
    "        interval = int((max_dist - start_dist) / 10)\n",
    "\n",
    "        dist_dic = {}\n",
    "        for dist in range(start_dist, max_dist, interval):\n",
    "            neighbors, weights = gaussian_weights(fim_geoid_local_var, dist)\n",
    "            w = libpysal.weights.W(neighbors, weights, silence_warnings=True)\n",
    "            bv_mi = esda.Moran_BV(fim_geoid_local_var['Class'], fim_geoid_local_var[census_name], w)\n",
    "            dist_dic[dist] = bv_mi.z_sim\n",
    "            \n",
    "        print(f\"Highest Z-Score at {max(dist_dic, key=dist_dic.get)} meters\")\n",
    "        optimal_dist = max(dist_dic, key=dist_dic.get)\n",
    "        \n",
    "        neighbors, weights = gaussian_weights(fim_geoid_local_var, optimal_dist)\n",
    "        w = libpysal.weights.W(neighbors, weights, silence_warnings=True)\n",
    "        bv_mi = esda.Moran_BV(fim_geoid_local_var['Class'], fim_geoid_local_var[census_name], w)          \n",
    "        bv_lm = esda.Moran_Local_BV(fim_geoid_local_var['Class'], fim_geoid_local_var[census_name], w, seed=17)\n",
    "            \n",
    "        # Enter results of Bivariate LISA into each census region\n",
    "        lm_dict = {1: 'HH', 2: 'LH', 3: 'LL', 4: 'HL'}\n",
    "        for idx in range(fim_geoid_local_var.shape[0]):\n",
    "            if bv_lm.p_sim[idx] < 0.05:\n",
    "                fim_geoid_local_var.loc[idx, f'LISA_{new_col_name}'] = lm_dict[bv_lm.q[idx]]\n",
    "            else:\n",
    "                fim_geoid_local_var.loc[idx, f'LISA_{new_col_name}'] = 'Not_Sig'\n",
    "\n",
    "        fim_geoid_local_na = fim_geoid_local.loc[fim_geoid_local[census_name].isna(), ['Dam_ID', 'GEOID', 'Class', census_name, 'geometry']]\n",
    "        fim_geoid_local_na[f'LISA_{new_col_name}'] = 'NA'\n",
    "        fim_geoid_local_var = pd.concat([fim_geoid_local_var, fim_geoid_local_na]).reset_index(drop=True)       \n",
    "        fim_geoid_local = fim_geoid_local.merge(fim_geoid_local_var[['GEOID', f'LISA_{new_col_name}']], on='GEOID')\n",
    "        fim_geoid_local[f'dist_{new_col_name}'] = optimal_dist\n",
    "\n",
    "        # Enter Bivariate Moran's I result into each dam\n",
    "        dam_local[f'MI_{new_col_name}'] = bv_mi.I\n",
    "        dam_local[f'pval_{new_col_name}'] = bv_mi.p_z_sim\n",
    "        dam_local[f'dist_{new_col_name}'] = optimal_dist\n",
    "\n",
    "    return dam_local, fim_geoid_local\n",
    "\n",
    "\n",
    "def spatial_correlation(dam_id, fd_gdf, fim_geoid_gdf, census_dic, API_Key):\n",
    "    \n",
    "    # Retrieve census data from API\n",
    "    print(f\"{dam_id}: Step 2, 1/2, Retrieving census data\")\n",
    "    fim_geoid_gdf['GEOID_T'] = fim_geoid_gdf.apply(lambda x:x['GEOID'][0:11], axis=1)\n",
    "\n",
    "    # List of states that is associated with the dam failure\n",
    "    state_list = fim_geoid_gdf.apply(lambda x:x['GEOID'][0:2], axis=1).unique()\n",
    "\n",
    "    cols = list(census_dic.keys())\n",
    "    cols.append('GEOID_T')\n",
    "\n",
    "    attr_df = pd.DataFrame({'GEOID_T':fim_geoid_gdf['GEOID_T'].unique().tolist()})\n",
    "    for attr in census_dic.keys(): # attr: svi-related census abbriviation on the final table\n",
    "        if type(census_dic[attr]) == str:\n",
    "            temp_table = call_census_table(state_list, census_dic[attr], API_Key)\n",
    "            attr_df = attr_df.merge(temp_table, on='GEOID_T')\n",
    "            attr_df = attr_df.rename(columns={census_dic[attr]: attr})\n",
    "        else:\n",
    "            for table in census_dic[attr][0]: # Retrieve numerator variables\n",
    "                temp_table = call_census_table(state_list, table, API_Key)\n",
    "                attr_df = attr_df.merge(temp_table, on='GEOID_T')\n",
    "\n",
    "            temp_table = call_census_table(state_list, census_dic[attr][1], API_Key) # Retrieve denominator variable\n",
    "            attr_df = attr_df.merge(temp_table, on='GEOID_T')\n",
    "            \n",
    "            # Calculate the ratio of each variable\n",
    "            attr_df[attr] = attr_df[census_dic[attr][0]].sum(axis=1) / attr_df[census_dic[attr][1]] * 100\n",
    "\n",
    "        # Remove intermediate columns used for SVI related census calculation\n",
    "        attr_df = attr_df[attr_df.columns.intersection(cols)]\n",
    "        \n",
    "        # Replace not valid value (e.g., -666666) from census with nan value\n",
    "        attr_df[attr] = attr_df.apply(lambda x: float('nan') if x[attr] < 0 else x[attr], axis=1)\n",
    "\n",
    "    # Merge census data with fim_geoid_gdf\n",
    "    fim_geoid_gdf = fim_geoid_gdf.merge(attr_df, on='GEOID_T')\n",
    "\n",
    "    # Reproject fim_geoid to EPSG:5070, NAD83 / Conus Albers (meters)\n",
    "    fim_geoid_gdf = fim_geoid_gdf.to_crs(epsg=5070)\n",
    "       \n",
    "    # Calculate Bivariate Moran's I & Local Moran's I\n",
    "    print(f\"{dam_id}: Step 2, 2/2, Calculating Moran\\'s I and LISA\")\n",
    "    mi_gdf, lm_gdf = calculate_bivariate_Moran_I_and_LISA(dam_id, census_dic, fim_geoid_gdf, fd_gdf)\n",
    "\n",
    "    return mi_gdf, lm_gdf\n",
    "\n",
    "mi_gdf, lm_gdf = spatial_correlation(dam_id, fed_dams, fim_geoid_gdf, census_dic, API_Key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc9ab93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# str: single variable\n",
    "# list: [[To be summed and set as numerator], demonimator]  \n",
    "census_info = {\n",
    "                \"EP_POV150\" : [['S1701_C01_040E'], 'S1701_C01_001E'],\n",
    "                \"EP_UNEMP\"  : 'DP03_0009PE',\n",
    "                \"EP_HBURD\"  : [['S2503_C01_028E', 'S2503_C01_032E', 'S2503_C01_036E', 'S2503_C01_040E'], \n",
    "                            'S2503_C01_001E'],\n",
    "                \"EP_NOHSDP\" : 'S0601_C01_033E',\n",
    "                \"EP_UNINSUR\" : 'S2701_C05_001E',\n",
    "                \"EP_AGE65\" : 'S0101_C02_030E',\n",
    "                \"EP_AGE17\" : [['B09001_001E'], \n",
    "                            'S0601_C01_001E'],\n",
    "                \"EP_DISABL\" : 'DP02_0072PE',\n",
    "                \"EP_SNGPNT\" : [['B11012_010E', 'B11012_015E'], 'DP02_0001E'],\n",
    "                \"EP_LIMENG\" : [['B16005_007E', 'B16005_008E', 'B16005_012E', 'B16005_013E', 'B16005_017E', 'B16005_018E', \n",
    "                                'B16005_022E', 'B16005_023E', 'B16005_029E', 'B16005_030E', 'B16005_034E', 'B16005_035E',\n",
    "                                'B16005_039E', 'B16005_040E', 'B16005_044E', 'B16005_045E'], \n",
    "                            'B16005_001E'],\n",
    "                \"EP_MINRTY\" : [['DP05_0071E', 'DP05_0078E', 'DP05_0079E', 'DP05_0080E', \n",
    "                                'DP05_0081E', 'DP05_0082E', 'DP05_0083E'],\n",
    "                            'S0601_C01_001E'],\n",
    "                \"EP_MUNIT\" : [['DP04_0012E', 'DP04_0013E'], \n",
    "                            'DP04_0001E'],\n",
    "                \"EP_MOBILE\" : 'DP04_0014PE',\n",
    "                \"EP_CROWD\" : [['DP04_0078E', 'DP04_0079E'], \n",
    "                            'DP04_0002E'],\n",
    "                \"EP_NOVEH\" : 'DP04_0058PE',\n",
    "                \"EP_GROUPQ\": [['B26001_001E'], \n",
    "                            'S0601_C01_001E'],\n",
    "}\n",
    "plot_cols_lm = ['LISA_' + col.split('_')[1] for col in census_info.keys()]\n",
    "plot_cols_mi = ['MI_' + col.split('_')[1] for col in census_info.keys()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b65c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8846273",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_bivariate_moran_i_and_local_moran_i(dam_id, lm_gdf, mi_gdf):\n",
    "    fig, axes = plt.subplots(4, 4, figsize=(20, 20))\n",
    "    ax = axes.reshape(-1)\n",
    "\n",
    "    # Define geography related to each dam\n",
    "    dam_area = lm_gdf.loc[lm_gdf['Dam_ID'] == dam_id].reset_index()\n",
    "\n",
    "    inund_area = dam_area.loc[dam_area['Class'] > 0]\n",
    "    inund_area_union = inund_area.dissolve(by='Dam_ID')\n",
    "\n",
    "    dam_focus = mi_gdf.loc[mi_gdf['ID'] == dam_id].reset_index()\n",
    "    dam_focus = dam_focus.to_crs(epsg=5070)\n",
    "\n",
    "    # Plot maps\n",
    "    for m in range(16):\n",
    "        lisa_color = {'HH': 'red', 'LL': 'blue', 'HL': 'orange', 'LH': 'skyblue', 'Not_Sig': 'white'}\n",
    "\n",
    "        for key in lisa_color.keys():\n",
    "            lm_gdf.loc[(lm_gdf[f'{plot_cols_lm[m]}'] == key) & (lm_gdf['Dam_ID'] == dam_id)].plot(ax=ax[m], color=lisa_color[key], legend=True)\n",
    "\n",
    "        ax[m].set_title(label=f\"{plot_cols_lm[m]} ({round(dam_focus[plot_cols_mi[m]].values[0], 2)})\", fontsize=24)\n",
    "\n",
    "        inund_area_union.boundary.plot(ax=ax[m], color='black', lw=1)\n",
    "        ax[m].get_xaxis().set_visible(False)\n",
    "        ax[m].get_yaxis().set_visible(False)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "# lm = gpd.read_file(f'./MH_F_Results_1/{iter_num}/MH_F_lm.geojson')\n",
    "plot_bivariate_moran_i_and_local_moran_i(dam_id, lm_gdf, mi_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e639a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
